import { Redis } from '@upstash/redis'
import {
  type LanguageModelV3,
  type LanguageModelV3Middleware,
  type LanguageModelV3StreamPart,
} from '@ai-sdk/provider'
import { obscured } from 'obscured'
import { EnvConfig } from '../../config/env.config.js'

import { simulateReadableStream } from 'ai'
import { TransformStream } from 'node:stream/web'

const redis = new Redis({
  url: obscured.value(EnvConfig.UPSTASH_REDIS_REST_URL),
  token: obscured.value(EnvConfig.UPSTASH_REDIS_REST_TOKEN),
})

/**
 * LanguageModelMiddleware has two methods: wrapGenerate and wrapStream. wrapGenerate is called when using generateText and generateObject, while wrapStream is called when using streamText and streamObject.
 * For wrapGenerate, you can cache the response directly. Instead, for wrapStream, you cache an array of the stream parts, which can then be used with simulateReadableStream function to create a simulated ReadableStream that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the initialDelayInMs and chunkDelayInMs parameters of simulateReadableStream.
 * https://ai-sdk.dev/docs/advanced/caching
 */

export const cacheMiddleware: LanguageModelV3Middleware = {
  specificationVersion: 'v3',
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params)

    const cached = (await redis.get(cacheKey)) as Awaited<
      ReturnType<LanguageModelV3['doGenerate']>
    > | null

    if (cached !== null) {
      return {
        ...cached,
        response: {
          ...cached.response,
          timestamp: cached?.response?.timestamp
            ? new Date(cached?.response?.timestamp)
            : undefined,
        },
      }
    }

    const result = await doGenerate()

    await redis.set(cacheKey, result)

    return result
  },
  wrapStream: async ({ doStream, params }) => {
    const cacheKey = JSON.stringify(params)

    // Check if the result is in the cache
    const cached = await redis.get(cacheKey)

    // If cached, return a simulated ReadableStream that yields the cached result
    if (cached !== null) {
      // Format the timestamps in the cached response
      const formattedChunks = (cached as LanguageModelV3StreamPart[]).map((p) => {
        if (p.type === 'response-metadata' && p.timestamp) {
          return { ...p, timestamp: new Date(p.timestamp) }
        } else return p
      })
      return {
        stream: simulateReadableStream({
          initialDelayInMs: 0,
          chunkDelayInMs: 10,
          chunks: formattedChunks,
        }),
      }
    }

    // If not cached, proceed with streaming
    const { stream, ...rest } = await doStream()

    const fullResponse: LanguageModelV3StreamPart[] = []

    const transformStream = new TransformStream<
      LanguageModelV3StreamPart,
      LanguageModelV3StreamPart
    >({
      transform(chunk, controller) {
        fullResponse.push(chunk)
        controller.enqueue(chunk)
      },
      flush() {
        // Store the full response in the cache after streaming is complete
        redis.set(cacheKey, fullResponse)
      },
    })

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    }
  },
}
